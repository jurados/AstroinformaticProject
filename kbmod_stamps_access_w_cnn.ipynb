{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jurados/AstroinformaticProject/blob/main/kbmod_stamps_access_w_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5573c371-9ff2-48b3-9921-00f29c364a80",
      "metadata": {
        "id": "5573c371-9ff2-48b3-9921-00f29c364a80"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0386aec0-bc52-4980-8382-ba3982b63f23",
      "metadata": {
        "id": "0386aec0-bc52-4980-8382-ba3982b63f23"
      },
      "source": [
        "# KBMOD Coadd Stamps\n",
        "\n",
        "## README\n",
        "\n",
        "The following data was generated for use as part of the [KBMOD](https://github.com/dirac-institute/kbmod) project, for the purpose of creating a machine learning model to filter out false positve results from the KBMOD search. The data set is divided into two sets of data, the true positive and false positive, with each set also divided into the training, validation, and test sets for ML purposes.\n",
        "\n",
        "### True Positive\n",
        "\n",
        "This dataset was generated based on the synthetic objects inserted into the DEEP data by Pedro Bernardinelli, and then cutout from the survey dataset by Steven Stetzler. The original stamp cutouts are available on hyak at `/gscratch/dirac/DEEP/collab/fakes_cutouts/data/*/npy` (the `*` being the id of the fake object in DEEP).\n",
        "\n",
        "### False Positive\n",
        "\n",
        "This dataset was generated by running KBMOD on the DEEP data, but setting the set of serach angles to be roughly perpendicular to the ecliptic. This allows us to semi-safely assume that any search results are not real or inserted objects (i.e., while some real or synthetic objects may have been included in this set, the ratio to bad results is small enough to not be of concern for machine learning purposes).\n",
        "\n",
        "### Data Transformation\n",
        "\n",
        "Each source observation in the above set was ran through a series of random transformations, to enlarge the dataset and to capture all possible features of both sets. Those transformations were:\n",
        "- select a random subset of the total set to coadd, with a minimum number of observations set to 25\n",
        "    - so if the N number of observations is > 25 then the set of stamps used in coadding is of length [25,N]\n",
        "- potentially offset the center pixel of each stamp by one in either direction, for both the X and Y in pixel space\n",
        "    - so the set x_offset = [-1, 0, 1] and y_offset = [-1, 0, 1], creating nine potential offset states around the \"actual\" center of the object/results.\n",
        "- potentially mirror around both the x and y axis\n",
        "- potentially rotate 90 degrees around the center N times, where N is [0-3]\n",
        "\n",
        "### Data Format\n",
        "\n",
        "The data arrays are stored as numpy `.npy` files, and can be loaded with `x = np.load(PATH)`.\n",
        "\n",
        "The shape of the data is `(N, 3, 21, 21)`, where N is the number of individual stamps in each set. The second dimmension corresponds to the type of coadd performed.\n",
        "- index 0 = median coadd stamp\n",
        "- index 1 = mean coadd stamp\n",
        "- index 2 = sum coadd stamp.\n",
        "Side note: the true positive stamp files also have an index 3, which corresponds to the recently added variance weighted coadd stamps. However, we found some discrepancies in those stamps and decided not to generate them for the false positive stamp. You can filter out this fourth column from the true positive set by slicing the array like [:,:3,:,:].\n",
        "\n",
        "Each coadd stamp has the shape (21,21).\n",
        "\n",
        "## Data\n",
        "\n",
        "Here are the preloaded data sets. If you want to generate your own train, test, and validation sets, you can load in the `true_positive_stamps_full.npy` and `false_positive_stamps_full.npy` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c0qAhs4bMoVX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "c0qAhs4bMoVX",
        "outputId": "2c6baef7-9319-4c03-9591-9cb4cb94836e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-dd2faeeb38ac>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/(POSGRADO) Universidad de Chile/ELECTIVE - Astroinformatic/Project'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd /content/gdrive/MyDrive/(POSGRADO) Universidad de Chile/ELECTIVE - Astroinformatic/Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f14e25a-e2b9-4484-ac34-0ea79b4de86d",
      "metadata": {
        "id": "2f14e25a-e2b9-4484-ac34-0ea79b4de86d"
      },
      "outputs": [],
      "source": [
        "data_dir = \"./kbmod_ml_stamps/\"\n",
        "\n",
        "# TRUE POSITVE PATHS\n",
        "true_pos_full  = os.path.join(data_dir, \"true_positive_stamps_full.npy\")\n",
        "true_pos_train = os.path.join(data_dir, \"true_train_stamps.npy\")\n",
        "true_pos_valid = os.path.join(data_dir, \"true_valid_stamps.npy\")\n",
        "true_pos_test  = os.path.join(data_dir, \"true_test_stamps.npy\")\n",
        "\n",
        "# FALSE POSITVE PATHS\n",
        "false_pos_full  = os.path.join(data_dir, \"false_positive_stamps_full.npy\")\n",
        "false_pos_train = os.path.join(data_dir, \"false_train_stamps.npy\")\n",
        "false_pos_valid = os.path.join(data_dir, \"false_valid_stamps.npy\")\n",
        "false_pos_test  = os.path.join(data_dir, \"false_test_stamps.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e32e273-2458-46f0-bafb-2f7888c90ffd",
      "metadata": {
        "id": "6e32e273-2458-46f0-bafb-2f7888c90ffd"
      },
      "outputs": [],
      "source": [
        "true_train = np.load(true_pos_train)[:,:3,:,:] # this indexing isn't strictly needed if you're just grabbing one of the other coadd type columns, see below cell.\n",
        "true_test  = np.load(true_pos_test)[:,:3,:,:]\n",
        "\n",
        "false_train = np.load(false_pos_train)\n",
        "true_train.shape, false_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19432ba5-ebe0-459c-ae2f-7d7f12c55020",
      "metadata": {
        "id": "19432ba5-ebe0-459c-ae2f-7d7f12c55020"
      },
      "outputs": [],
      "source": [
        "true_train_median  = true_train[:,0,:,:]\n",
        "false_train_median = false_train[:,0,:,:]\n",
        "\n",
        "true_train_median.shape, false_train_median.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a14d06-5c3e-4c0e-95f4-f9b967bf0a0a",
      "metadata": {
        "id": "58a14d06-5c3e-4c0e-95f4-f9b967bf0a0a"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "If you want to visualize what certain stamps look like, you can use `plt.imshow` and pass in the desired row.\n",
        "\n",
        "Side note on visualizing the true positive stamps: we specifically weakened the signal on some of the observations to simulate a wider range of data. Visualizing some of the stamps might seem like mostly random noise, but I do promise there is signal there. Some of the time the PSF isn't visible in the median stamp but is seen in the mean or sum, or vice versa. I'm still messing around with the dataset so if we're running into too much low-SNR contimination in this set, we can raise the minimum observations in the selection algorithm and rerun everything, it only takes about 20 minutes to do :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b085b3-04de-4921-8cf0-0d03a18e1a5a",
      "metadata": {
        "id": "f5b085b3-04de-4921-8cf0-0d03a18e1a5a"
      },
      "outputs": [],
      "source": [
        "def plot_true_false(true, false):\n",
        "    '''Plot a random set of true and false images'''\n",
        "    fig_0,ax_0 = plt.subplots(nrows=2,ncols=5,figsize=[8,4])\n",
        "    fig_1,ax_1 = plt.subplots(nrows=2,ncols=5,figsize=[8,4])\n",
        "    ax_0 = ax_0.reshape(-1)\n",
        "    ax_1 = ax_1.reshape(-1)\n",
        "    true_int_list = np.random.choice(len(true),10,replace=False)\n",
        "    false_int_list = np.random.choice(len(false),10,replace=False)\n",
        "    for i, ax in enumerate(ax_0):\n",
        "        ax.imshow(true[true_int_list[i]])\n",
        "    for i, ax in enumerate(ax_1):\n",
        "        ax.imshow(false[false_int_list[i]])\n",
        "    fig_0.suptitle('True',fontsize=16)\n",
        "    fig_1.suptitle('False',fontsize=16)\n",
        "    fig_0.tight_layout()\n",
        "    fig_1.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48d5ffc-01c4-4af4-a3b1-785540773a31",
      "metadata": {
        "id": "b48d5ffc-01c4-4af4-a3b1-785540773a31"
      },
      "outputs": [],
      "source": [
        "plot_true_false(true_train[:,0,:,:], false_train[:,0,:,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac3c3f6-c6dd-4a24-818b-265ea9dc64d8",
      "metadata": {
        "id": "5ac3c3f6-c6dd-4a24-818b-265ea9dc64d8"
      },
      "source": [
        "## Training\n",
        "\n",
        "I've included a simple model here as jumping off point. Go wild!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9505f56c-87eb-4409-81d7-faa96010f523",
      "metadata": {
        "id": "9505f56c-87eb-4409-81d7-faa96010f523"
      },
      "outputs": [],
      "source": [
        "false_pos_valid = os.path.join(data_dir, \"false_valid_stamps.npy\")\n",
        "_a = np.load(false_pos_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MUf1klgStsYz",
      "metadata": {
        "id": "MUf1klgStsYz"
      },
      "outputs": [],
      "source": [
        "# combine data\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "def stack_data(f, t):\n",
        "    input_stamps = np.vstack([f, t])\n",
        "    stamp_class = np.zeros(len(f) + len(t))\n",
        "    stamp_class[len(f):] = 1\n",
        "\n",
        "    rand_state = np.random.RandomState(32)\n",
        "    idx = rand_state.permutation(len(input_stamps))\n",
        "    input_stamps = input_stamps[idx]\n",
        "    stamp_class = stamp_class[idx]\n",
        "    stamp_class = to_categorical(stamp_class)\n",
        "    #stamp_class = stamp_class.astype(int)\n",
        "    return input_stamps, stamp_class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "false_train_median.shape, true_train_median.shape"
      ],
      "metadata": {
        "id": "ll0vk07fuWf6"
      },
      "id": "ll0vk07fuWf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd9ac6c-c380-4161-a4e4-3e259f9323af",
      "metadata": {
        "id": "efd9ac6c-c380-4161-a4e4-3e259f9323af"
      },
      "outputs": [],
      "source": [
        "true_valid_median  = np.load(true_pos_valid)[:,0,:,:]\n",
        "false_valid_median = np.load(false_pos_valid)[:,0,:,:]\n",
        "\n",
        "true_test_median = np.load(true_pos_test)[:,0,:,:]\n",
        "false_test_median = np.load(false_pos_test)[:,0,:,:]\n",
        "\n",
        "train_data, train_class = stack_data(false_train_median, true_train_median)\n",
        "val_data, val_class     = stack_data(false_valid_median, true_valid_median)\n",
        "test_data, test_class   = stack_data(false_test_median, true_test_median)\n",
        "\n",
        "# Reshape [Channel, Heigh, Width]\n",
        "train_data = train_data.reshape(-1,1,21,21)\n",
        "val_data   = val_data.reshape(-1,1,21,21)\n",
        "test_data  = test_data.reshape(-1,1,21,21)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F1MZ2vQPwS-Q",
      "metadata": {
        "id": "F1MZ2vQPwS-Q"
      },
      "outputs": [],
      "source": [
        "print('Train_data shape:',train_data.shape, 'Train_class shape:',train_class.shape)\n",
        "print(train_class[:10])\n",
        "print('Val_data shape:',val_data.shape,'Val_class shape:',val_class.shape)\n",
        "print('Test_data shape:',test_data.shape,'Test_class shape:',test_class.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L19ES_PLxYuW",
      "metadata": {
        "id": "L19ES_PLxYuW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "try:\n",
        "    import lightning as L\n",
        "except:\n",
        "    !pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iPyOTwUKPo-Q",
      "metadata": {
        "id": "iPyOTwUKPo-Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter(\"logs/\")\n",
        "\n",
        "import lightning as L\n",
        "from torchsummary import summary\n",
        "from torchmetrics.classification import Accuracy, Recall\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N0eGFgzX5ZUI",
      "metadata": {
        "id": "N0eGFgzX5ZUI"
      },
      "outputs": [],
      "source": [
        "initial_settings = {\n",
        "    'batch_size': 512,\n",
        "    'learning_rate': 1e-3,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gYLubGT5CrzO",
      "metadata": {
        "id": "gYLubGT5CrzO"
      },
      "source": [
        "[Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
        "\n",
        "Shape:\n",
        "- Input:  $(N,C_{in},H_{in}, W_{in})$ or $(C_{in},H_{in}, W_{in})$\n",
        "- Output: $(N,C_{out},H_{out}, W_{out})$ or $(C_{out},H_{out}, W_{out})$\n",
        "\n",
        "Parameters:\n",
        "- `in_channels`\n",
        "- `out_channels`\n",
        "- `kernel_size`\n",
        "- `stride`: Default value is 1.\n",
        "- `padding`: Default value is 0.\n",
        "- `dilation`: Default value is 1.\n",
        "- `groups`: controls the connections between inputs and outputs.\n",
        "- `bias`: If `True`, adds a learnable bias to the output.\n",
        "- `padding_mode`\n",
        "\n",
        "\n",
        "$$ H_{out}=\\left[\\frac{H_{in}+2 \\times \\text{padding}[0]-\\text{dilation}[0] \\times (\\text{kernel_size}[0]- 1)- 1}{\\text{stride}[0]} +1 \\right]\n",
        "$$\n",
        "$$ W_{out}=\\left[\\frac{W_{in}+2 \\times \\text{padding}[1]-\\text{dilation}[1] \\times (\\text{kernel_size}[1]- 1)- 1}{\\text{stride}[0]} +1 \\right]\n",
        "$$\n",
        "\n",
        "Parameters:\n",
        "- `kernel_size`\n",
        "- `stride`: Default value is `kernel_size`.\n",
        "- `padding`\n",
        "- `dilation`\n",
        "- `return_indices`: If `True`, will return te max indices along with the outputs. Useful for `torch.nn.MaxUnpool2d` later.\n",
        "- `ceil_mode`: when `True`, will use _ceil_ instead of _floor_ to compute the output shape.\n",
        "\n",
        "\n",
        "[`MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
        "\n",
        "Shape:\n",
        "- Input:  $(N,C,H_{in}, W_{in})$ or $(C,H_{in}, W_{in})$\n",
        "- Output: $(N,C,H_{out}, W_{out})$ or $(C,H_{out}, W_{out})$\n",
        "\n",
        "Parameters:\n",
        "- `kernel_size`\n",
        "- `stride`: Default value is `kernel_size`.\n",
        "- `padding`\n",
        "- `dilation`\n",
        "- `return_indices`: If `True`, will return te max indices along with the outputs. Useful for `torch.nn.MaxUnpool2d` later.\n",
        "- `ceil_mode`: when `True`, will use _ceil_ instead of _floor_ to compute the output shape.\n",
        "\n",
        "$$ H_{out}=\\left[\\frac{H_{in}+2 \\times \\text{padding}[0]-\\text{dilation}[0] \\times (\\text{kernel_size}[0]- 1)- 1}{\\text{stride}[0]} +1 \\right]\n",
        "$$\n",
        "$$ W_{out}=\\left[\\frac{W_{in}+2 \\times \\text{padding}[1]-\\text{dilation}[1] \\times (\\text{kernel_size}[1]- 1)- 1}{\\text{stride}[0]} +1 \\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XntDWbzPLY3d",
      "metadata": {
        "id": "XntDWbzPLY3d"
      },
      "source": [
        "### Model V0: Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b33npA29o1u",
      "metadata": {
        "id": "0b33npA29o1u"
      },
      "outputs": [],
      "source": [
        "class Basic(L.LightningModule):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        #self.save_hyperparameters()\n",
        "        self.model_layers = nn.Sequential(\n",
        "            # Input (Cin=1, Hin=21, Win=21, kenel_size=3, stride=1, padding=0, dilation=1)\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3),\n",
        "            # Output (Cout=8, Hin=18, Win=18)\n",
        "            nn.ReLU(),\n",
        "            # Output (Cout=8, Hin=18, Win=18)\n",
        "            nn.BatchNorm2d(num_features=8),\n",
        "            # Output (Cout=8, Hin=18, Win=18)\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # Output (Cout=8, Hin=9, Win=9)\n",
        "            nn.Dropout(p=0.25),\n",
        "            # Output (Cout=8, Hin=9, Win=9)\n",
        "            nn.Flatten(),\n",
        "            # Output (8*9*9)\n",
        "            nn.Linear(in_features=8*9*9, out_features=64),\n",
        "        )\n",
        "        self.activation = nn.Sigmoid() if n_classes == 1 else nn.Softmax(dim=1)\n",
        "        self.fc_out = nn.Linear(in_features=64, out_features=n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape) # [Batch, Heigh, Width, Channel]\n",
        "        x = x.view(x.size(0), 1, 21, 21)\n",
        "        #x = x.permute(0,3,1,2) # # [Batch, Channel, Heigh, Width]\n",
        "        x = self.model_layers(x)\n",
        "        x = self.fc_out(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "model = Basic(n_classes=2).to(device) # Move the model to the GPU\n",
        "summary(model, (21, 21, 1))  # Now call summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ybO_brLcJ5",
      "metadata": {
        "id": "c4ybO_brLcJ5"
      },
      "source": [
        "### Model V1: ResNet-56"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MSc7CRRiB4uW",
      "metadata": {
        "id": "MSc7CRRiB4uW"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    change_size: flag to indicate if activations are within normal residual blocks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride, change_size=True):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, stride=stride)\n",
        "        self.bn1   = nn.BatchNorm2d(out_channel)\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, stride=1)\n",
        "        self.bn2   = nn.BatchNorm2d(out_channel)\n",
        "        self.change_size = change_size\n",
        "        # for changing activation map sizes\n",
        "        if change_size:\n",
        "            self.residual = nn.Sequential(nn.Conv2d(in_channel, out_channel,\n",
        "                                            kernel_size=1, stride=stride),\n",
        "                                nn.BatchNorm2d(out_channel)\n",
        "            )\n",
        "\n",
        "    def forward(self,x):\n",
        "        identity = x if not self.change_size else self.residual(x)\n",
        "        y = F.relu(self.bn1(self.conv1(x)))\n",
        "        y = self.bn2(self.conv2(y))\n",
        "        y += identity\n",
        "\n",
        "        return F.relu(y)\n",
        "\n",
        "\n",
        "class ResNet56(nn.Module):\n",
        "    \"\"\"RestNet56\n",
        "    \"\"\"\n",
        "    def __init__(self, n=9, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1  = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1    = nn.BatchNorm2d(16)\n",
        "        self.block1 = self.create_block(n=9, in_channel=16, out_channel=16, stride=1, change_size=False)\n",
        "        self.block2 = self.create_block(n=9, in_channel=16, out_channel=32, stride=2, change_size=True)\n",
        "        self.block3 = self.create_block(n=9, in_channel=32, out_channel=64, stride=2, change_size=True)\n",
        "        self.fc     = nn.Linear(64, num_classes)\n",
        "        self.activation = nn.Sigmoid() if num_classes == 1 else nn.Softmax(dim=1)\n",
        "\n",
        "    def create_block(self, n, in_channel, out_channel, stride, change_size=True):\n",
        "        block = [ResidualBlock(in_channel, out_channel, stride, change_size)]\n",
        "        for i in range(n-1):\n",
        "            block.append(ResidualBlock(out_channel, out_channel, stride=1, change_size=False))\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), 1, 21, 21)\n",
        "        y = F.relu(self.bn1(self.conv1(x)))\n",
        "        y = self.block1(y)\n",
        "        y = self.block2(y)\n",
        "        y = self.block3(y)\n",
        "        y = F.adaptive_avg_pool2d(input=y, output_size=1)\n",
        "        y = self.fc(y.view(y.size(0), -1))\n",
        "        y = self.activation(y)\n",
        "        return y\n",
        "\n",
        "model = ResNet56().to(device)   # Move the model to the GPU\n",
        "summary(model, (21, 21, 1))  # Now call summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Lo1PsfdLmq7",
      "metadata": {
        "id": "_Lo1PsfdLmq7"
      },
      "source": [
        "### Model V2: ResNet-50"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RnUlxCVSSs19",
      "metadata": {
        "id": "RnUlxCVSSs19"
      },
      "source": [
        "Here, model_parameters[‘resnet50’] = ([64,128,256,512],[3,4,6,3],4,True) represents the parameters for resnet-50 where\n",
        "\n",
        "[64,128,256,512] -> channels in each intermediate block\n",
        "[3,4,6,3] -> # repeatition for Bottlenecks in each block\n",
        "4 -> expansion_factor. Note that 64 turns to 256, 128 to 512. All the resnet layers use the same expansion factor.\n",
        "True -> create Bottleneck layer status. True only for ResNet 50+\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zBNdk-hRNEmS",
      "metadata": {
        "id": "zBNdk-hRNEmS"
      },
      "outputs": [],
      "source": [
        "# This model is upload\n",
        "import torchvision\n",
        "\n",
        "class ResNet50(L.LightningModule):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.resnet50 = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
        "\n",
        "        # Dont change the parameters\n",
        "        # Keep without change the parameters\n",
        "        for param in self.resnet50.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.resnet50.conv1 = nn.Conv2d(in_channels=1, out_channels=self.resnet50.conv1.out_channels,\n",
        "                                        kernel_size=self.resnet50.conv1.kernel_size, stride=self.resnet50.conv1.stride,\n",
        "                                        padding=self.resnet50.conv1.padding, bias=False) # change input channels to 21\n",
        "        self.resnet50.fc = nn.Linear(in_features=self.resnet50.fc.in_features, out_features=num_classes)\n",
        "        self.activation = nn.Sigmoid() if num_classes == 1 else nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), 1, 21, 21)\n",
        "        y = self.resnet50(x)\n",
        "        y = self.activation(y)\n",
        "        return y\n",
        "\n",
        "model = ResNet50(num_classes=2).to(device=device)\n",
        "#summary(model, (1, 21, 21))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(5, 1, 21, 21).to(device=device)  # Batch de 5 imágenes\n",
        "y_hat = model(x)\n",
        "print(y_hat)"
      ],
      "metadata": {
        "id": "_1PZu26F01f0"
      },
      "id": "_1PZu26F01f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K238LMKSLpXd",
      "metadata": {
        "id": "K238LMKSLpXd"
      },
      "outputs": [],
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride, expansion, is_Bottleneck):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.expansion = expansion\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.is_Bottleneck = is_Bottleneck\n",
        "\n",
        "        if self.in_channels == self.out_channels*self.expansion:\n",
        "            self.identity = True\n",
        "        else:\n",
        "            self.identity = False\n",
        "            projection_layer = []\n",
        "            projection_layer.append(nn.Conv2d(self.in_channels, self.out_channels*self.expansion, kernel_size=1, stride=stride))\n",
        "            # Only Conv -> BN and no Relu\n",
        "            # projection_layer.append(nn.ReLu())\n",
        "            self.projection = nn.Sequential(*projection_layer)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # is_Bottleneck = True for all ResNet\n",
        "        if self.is_Bottleneck:\n",
        "            # Bottleneck\n",
        "            # 1x1\n",
        "            self.conv1 = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=1)\n",
        "            self.bn1 = nn.BatchNorm2d(self.out_channels)\n",
        "            # 3x3\n",
        "            self.conv2 = nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "            self.bn2 = nn.BatchNorm2d(self.out_channels)\n",
        "            # 1x1\n",
        "            self.conv3 = nn.Conv2d(self.out_channels, self.out_channels  * self.expansion, kernel_size=1, stride=1)\n",
        "            self.bn3 = nn.BatchNorm2d(self.out_channels*self.expansion)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.is_Bottleneck:\n",
        "            # conv1x1 -> BN -> ReLU\n",
        "            x = self.relu(self.bn1(self.conv1(x)))\n",
        "            # conv3x3 -> BN -> ReLU\n",
        "            x = self.relu(self.bn2(self.conv2(x)))\n",
        "            # conv1x1 -> BN\n",
        "            x = self.bn3(self.conv3(x))\n",
        "\n",
        "        if self.identity:\n",
        "            x += identity\n",
        "        else:\n",
        "            x += self.projection(identity)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ModelV2(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet-50 model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(ModelV2, self).__init__()\n",
        "\n",
        "        self.channels_list = [64,128,256,512]\n",
        "        self.repeatition_list = [3,4,6,3]\n",
        "        self.expansion = 4\n",
        "        self.is_Bottleneck = True\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.block1 = self._make_blocks(64, self.channels_list[0], 3, stride=1, expansion=self.expansion, is_Bottleneck=self.is_Bottleneck)\n",
        "        self.block2 = self._make_blocks(self.channels_list[0]*self.expansion, self.channels_list[1], self.repeatition_list[1], stride=2, expansion=self.expansion, is_Bottleneck=self.is_Bottleneck)\n",
        "        self.block3 = self._make_blocks(self.channels_list[1]*self.expansion, self.channels_list[2], self.repeatition_list[2], stride=2, expansion=self.expansion, is_Bottleneck=self.is_Bottleneck)\n",
        "        self.block4 = self._make_blocks(self.channels_list[2]*self.expansion, self.channels_list[3], self.repeatition_list[3], stride=2, expansion=self.expansion, is_Bottleneck=self.is_Bottleneck)\n",
        "\n",
        "        self.average_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(self.channels_list[3]*self.expansion, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), 1, 21, 21)\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "\n",
        "        x = self.average_pool(x)\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_blocks(self, in_channels, out_channels, num_repeat, stride, expansion, is_Bottleneck):\n",
        "        layers = [Bottleneck(in_channels, out_channels, stride, expansion, is_Bottleneck)]\n",
        "        for i in range(1, num_repeat):\n",
        "            layers.append(Bottleneck(out_channels*expansion, out_channels, 1, expansion, is_Bottleneck))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "model = ModelV2(in_channels=1, num_classes=2).to(device)   # Move the model to the GPU\n",
        "summary(model, (21, 21, 1))  # Now call summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W6aEVL9zVQvH",
      "metadata": {
        "id": "W6aEVL9zVQvH"
      },
      "outputs": [],
      "source": [
        "class StampClassifier(L.LightningModule):\n",
        "    def __init__(self, num_classes, model: str):\n",
        "        super().__init__()\n",
        "\n",
        "        if model == 'basic':\n",
        "            self.model = Basic(num_classes)\n",
        "        elif model == 'resnet56':\n",
        "            self.model = ResNet56(num_classes)\n",
        "        elif model == 'resnet50':\n",
        "            self.model  = ResNet50(num_classes)\n",
        "\n",
        "        # Metrics\n",
        "        self.train_acc = Accuracy(task=\"multilabel\", num_labels=num_classes)\n",
        "        self.val_acc   = Accuracy(task=\"multilabel\", num_labels=num_classes)\n",
        "        self.test_acc  = Accuracy(task=\"multilabel\", num_labels=num_classes)\n",
        "\n",
        "        self.train_recall = Recall(task=\"multilabel\", num_labels=num_classes)\n",
        "        self.val_recall   = Recall(task=\"multilabel\", num_labels=num_classes)\n",
        "        self.test_recall  = Recall(task=\"multilabel\", num_labels=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self.model(x)\n",
        "        #print('y:', y[:5])\n",
        "        #print('y_hat:',y_hat[:5])\n",
        "\n",
        "        loss = self.loss_function()(y_hat, y)\n",
        "\n",
        "        # Log metrics\n",
        "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_accuracy\", self.train_acc(y_hat, y), on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_recall\", self.train_recall(y_hat, y), on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self.model(x)\n",
        "        loss = self.loss_function()(y_hat, y)\n",
        "\n",
        "        # Log metrics\n",
        "        self.log(\"val_loss\", loss, on_epoch=True)\n",
        "        self.log(\"val_accuracy\", self.val_acc(y_hat, y), on_epoch=True, prog_bar=True)\n",
        "        self.log(\"val_recall\", self.val_recall(y_hat, y), on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # this is the test loop\n",
        "        x, y = batch\n",
        "        y_hat = self.model(x)\n",
        "        loss = self.loss_function()(y_hat, y)\n",
        "        self.log(\"test_loss\", loss, on_epoch=True)\n",
        "        self.log(\"test_accuracy\", self.test_acc(y_hat, y), on_epoch=True)\n",
        "        self.log(\"test_recall\", self.test_recall(y_hat, y), on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=initial_settings['learning_rate'])\n",
        "        return optimizer\n",
        "\n",
        "    def loss_function(self):\n",
        "        return nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EM3j69DGOEap",
      "metadata": {
        "id": "EM3j69DGOEap"
      },
      "source": [
        "### Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BXSGHqy-HEEN",
      "metadata": {
        "id": "BXSGHqy-HEEN"
      },
      "outputs": [],
      "source": [
        "class StampsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data   = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        label  = self.labels[idx]\n",
        "        return sample, label\n",
        "\n",
        "train_dataset      = StampsDataset(train_data, train_class)\n",
        "validation_dataset = StampsDataset(val_data, val_class)\n",
        "test_dataset       = StampsDataset(test_data, test_class)\n",
        "train_dataloader   = DataLoader(train_dataset, batch_size=initial_settings['batch_size'], shuffle=True)\n",
        "val_dataloader     = DataLoader(validation_dataset, batch_size=initial_settings['batch_size'], shuffle=False)\n",
        "test_dataloader    = DataLoader(test_dataset, batch_size=initial_settings['batch_size'], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, train_class.shape, test_data.shape, test_class.shape"
      ],
      "metadata": {
        "id": "nkhtfDvNtXKz"
      },
      "id": "nkhtfDvNtXKz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(train_dataloader))\n",
        "x.shape, y.shape, y[:5]"
      ],
      "metadata": {
        "id": "FvpW3MrKITSf"
      },
      "id": "FvpW3MrKITSf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(test_dataloader))\n",
        "x.shape, y.shape, y[:10]"
      ],
      "metadata": {
        "id": "_flg0M5Yp_Ap"
      },
      "id": "_flg0M5Yp_Ap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E9HnCYeXM0Q-",
      "metadata": {
        "id": "E9HnCYeXM0Q-"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch.callbacks import Callback\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = {\n",
        "            \"train_loss\": [],\n",
        "            \"train_accuracy\": [],\n",
        "            \"train_recall\": [],\n",
        "            \"val_loss\": [],\n",
        "            \"val_accuracy\": [],\n",
        "            \"val_recall\": [],\n",
        "            \"test_loss\": [],\n",
        "            \"test_accuracy\": [],\n",
        "            \"test_recall\": [],\n",
        "        }\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        # Retrieve and save training metrics\n",
        "        train_loss = trainer.callback_metrics.get(\"train_loss\")\n",
        "        train_accuracy = trainer.callback_metrics.get(\"train_accuracy\")\n",
        "\n",
        "        self.metrics[\"train_loss\"].append(train_loss.item())\n",
        "        self.metrics[\"train_accuracy\"].append(train_accuracy.item())\n",
        "        self.metrics[\"train_recall\"].append(trainer.callback_metrics.get(\"train_recall\").item())\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        # Retrieve and save validation metrics\n",
        "        val_loss = trainer.callback_metrics.get(\"val_loss\")\n",
        "        val_accuracy = trainer.callback_metrics.get(\"val_accuracy\")\n",
        "        self.metrics[\"val_loss\"].append(val_loss.item())\n",
        "        self.metrics[\"val_accuracy\"].append(val_accuracy.item())\n",
        "        self.metrics[\"val_recall\"].append(trainer.callback_metrics.get(\"val_recall\").item())\n",
        "\n",
        "    def on_test_epoch_end(self, trainer, pl_module):\n",
        "        # Retrieve and save test metrics\n",
        "        test_loss = trainer.callback_metrics.get(\"test_loss\")\n",
        "        test_accuracy = trainer.callback_metrics.get(\"test_accuracy\")\n",
        "        self.metrics[\"test_loss\"].append(test_loss.item())\n",
        "        self.metrics[\"test_accuracy\"].append(test_accuracy.item())\n",
        "        self.metrics[\"test_recall\"].append(trainer.callback_metrics.get(\"test_recall\").item())\n",
        "\n",
        "    def get_metrics(self):\n",
        "        \"\"\"Returns the metrics dictionary for plotting.\"\"\"\n",
        "        return self.metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VaLpZpPCD3YM",
      "metadata": {
        "id": "VaLpZpPCD3YM"
      },
      "outputs": [],
      "source": [
        "models = ['basic', 'resnet56', 'resnet50']\n",
        "#models = ['resnet56']#, 'resnet50']\n",
        "results_metrics = {}\n",
        "results_predictions = {}\n",
        "\n",
        "for model in models:\n",
        "    classifier = StampClassifier(num_classes=2, model=model)\n",
        "    metrics_callback = MetricsCallback()\n",
        "\n",
        "    trainer = L.Trainer(max_epochs=20, callbacks=[metrics_callback])\n",
        "    trainer.fit(classifier, train_dataloader, val_dataloader)\n",
        "\n",
        "    # Retrieve metrics\n",
        "    metrics = metrics_callback.get_metrics()\n",
        "\n",
        "    # Store the results\n",
        "    results_metrics[model] = metrics\n",
        "\n",
        "    # Test the model and save predictions\n",
        "    probabilities = []\n",
        "    predictions   = []\n",
        "    true_labels   = []\n",
        "\n",
        "    classifier.eval()\n",
        "    with torch.inference_mode():\n",
        "        for batch in test_dataloader:\n",
        "            x, y = batch\n",
        "            y_hat = classifier(x)\n",
        "            preds = (y_hat >= 0.5).float()\n",
        "            probabilities.extend(y_hat.cpu().numpy())\n",
        "            predictions.extend(preds.cpu().numpy())  # Almacena las predicciones binarizadas\n",
        "            true_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    # Convertir etiquetas y predicciones a índices solo para la matriz de confusión\n",
        "    true_labels_indices = [np.argmax(label) for label in true_labels]  # `[1, 0]` → `0`, `[0, 1]` → `1`\n",
        "    predictions_indices = [np.argmax(pred) for pred in predictions]  # `[1, 0]` → `0`, `[0, 1]` → `1`\n",
        "\n",
        "    # Calcular la matriz de confusión\n",
        "    cm = confusion_matrix(true_labels_indices, predictions_indices, labels=[0, 1])\n",
        "\n",
        "    # Save predictions and true labels for further analysis\n",
        "    results_predictions[model] = {\n",
        "        'probabilities': probabilities,\n",
        "        \"predictions\": predictions,\n",
        "        \"true_labels\": true_labels,\n",
        "        \"confusion_matrix\": cm,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UGiI7cq0avR-",
      "metadata": {
        "id": "UGiI7cq0avR-"
      },
      "outputs": [],
      "source": [
        "for _, (model, metrics) in enumerate(results_metrics.items()):\n",
        "    print(f\"Model: {model}\")\n",
        "    print(f\"Train Loss: {np.round(metrics['train_loss'],decimals=3)}\")\n",
        "    print(f\"Train Accuracy: {np.round(metrics['train_accuracy'],decimals=3)}\")\n",
        "    print(f\"Train Recall: {np.round(metrics['train_recall'],decimals=3)}\")\n",
        "    print(f\"Validation Loss: {np.round(metrics['val_loss'],decimals=3)}\")\n",
        "    print(f\"Validation Accuracy: {np.round(metrics['val_accuracy'],decimals=3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SczVzScfZgD3",
      "metadata": {
        "id": "SczVzScfZgD3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=[10, 10])\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "for _, (model, metrics) in enumerate(results_metrics.items()):\n",
        "    #ls = {'basic': 'solid', 'resnet56': 'dashed', 'resnet50': 'dotted'}[model]\n",
        "    color = {'basic': 'C0', 'resnet56': 'C1', 'resnet50': 'C2'}[model]\n",
        "    ax[0].plot(metrics['train_loss'], color=color, label=f'Training Loss - {model}')\n",
        "    ax[0].plot(metrics['val_loss'], color=color, ls='dashed', label=f'Validation Loss - {model}')\n",
        "\n",
        "    ax[1].plot(metrics['train_accuracy'], color=color, label=f'Training Accuracy - {model}')\n",
        "    ax[1].plot(metrics['val_accuracy'], color=color, ls='dashed', label=f'Validation Accuracy - {model}')\n",
        "\n",
        "    ax[2].plot(metrics['train_recall'], color=color, label=f'Training Recall - {model}')\n",
        "    ax[2].plot(metrics['val_recall'], color=color, ls='dashed', label=f'Validation Recall - {model}')\n",
        "\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_title('Loss Curves')\n",
        "ax[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))  # optionally show legend\n",
        "\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].set_title('Accuracy Curves')\n",
        "ax[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))  # optionally show legend\n",
        "\n",
        "ax[2].set_xlabel('Epoch')\n",
        "ax[2].set_ylabel('Recall')\n",
        "ax[2].set_title('Recall Curves')\n",
        "ax[2].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.savefig('./loss_accuracy.png', dpi=300)\n",
        "plt.show()       # display the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D8_ICtjekXXU",
      "metadata": {
        "id": "D8_ICtjekXXU"
      },
      "outputs": [],
      "source": [
        "len(results_predictions['basic']), results_predictions['basic'].keys(), len(results_predictions['basic']['probabilities'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_predictions['basic']['predictions'][:10]"
      ],
      "metadata": {
        "id": "vzfU01yRzxZ4"
      },
      "id": "vzfU01yRzxZ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 20\n",
        "for batch in test_dataloader:\n",
        "    x, y = batch\n",
        "    test_images = x[:n,...]\n",
        "    true_labels = y[:n,...]\n",
        "    break\n",
        "\n",
        "num_cols = 5\n",
        "num_rows = (n + num_cols - 1) // num_cols  # Compute rows dynamically\n",
        "\n",
        "for model in models:\n",
        "    fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=[16, 12])\n",
        "    fig.suptitle(f'Test Images Prob Model:{model}')\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    # Retrieve the probabilities for the current model\n",
        "    probabilities = np.array(results_predictions[model]['probabilities'][:n])\n",
        "    predictions = (probabilities >= 0.5).astype(int)  # Binarize predictions\n",
        "\n",
        "    #print(probabilities)\n",
        "\n",
        "    # Remove unused axes\n",
        "    for tmp_ax in ax[len(test_images[:n]):]:\n",
        "        tmp_ax.set_axis_off()\n",
        "\n",
        "    for i, image in enumerate(test_images[:n]):\n",
        "        is_correct = np.array_equal(predictions[i], true_labels[i].cpu().numpy())\n",
        "        title_color = \"green\" if is_correct else \"red\"\n",
        "        ax[i].imshow(image[0], cmap=\"viridis\")  # Use channel 0 for grayscale\n",
        "        ax[i].set_title(\n",
        "            f\"Pred: {predictions[i].tolist()} (True: {[int(x) for x in true_labels[i].tolist()]})\\nProb: {probabilities[i,1]:.2f}\",\n",
        "            color=title_color,\n",
        "        )\n",
        "        ax[i].set_axis_on()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'./test_images_prob_{model}.png', dpi=300)  # Use channel 0 for grayscale\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "werkKwMiV9tx"
      },
      "id": "werkKwMiV9tx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally, plot the confusion matrix\n",
        "for model in models:\n",
        "    cm = results_predictions[model]['confusion_matrix']\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"FP\", \"TP\"])\n",
        "    disp.plot(cmap=\"Blues\")\n",
        "    plt.title(f\"Confusion Matrix for {model}\")\n",
        "    plt.savefig(f'./confusion_matrix_{model}.png', dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sbLpxyYPbWv1"
      },
      "id": "sbLpxyYPbWv1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5KoAIlW4aQIV"
      },
      "id": "5KoAIlW4aQIV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "Pg2ndWO7jpd3",
      "metadata": {
        "id": "Pg2ndWO7jpd3"
      },
      "source": [
        "below 1sigma or 2sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WcgRARGKtmTg",
      "metadata": {
        "id": "WcgRARGKtmTg"
      },
      "source": [
        "# Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924d759c-3ce9-4cb4-b573-c1b294e1241e",
      "metadata": {
        "id": "924d759c-3ce9-4cb4-b573-c1b294e1241e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2ad037-9fbf-4b26-ae0c-3fa4ba97df99",
      "metadata": {
        "id": "ce2ad037-9fbf-4b26-ae0c-3fa4ba97df99"
      },
      "outputs": [],
      "source": [
        "def simple_model(input_shape=(21,21,1), n_classes: int = 2):\n",
        "    model = tf.keras.models.Sequential(name='simple')\n",
        "    model.add(tf.keras.layers.Conv2D(8, (3,3), activation='relu', input_shape=input_shape, name='conv1'))\n",
        "    model.add(tf.keras.layers.BatchNormalization(axis = 3, name = 'bn1'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.25))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(64, activation='relu', name='fc_1'))\n",
        "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
        "    model.add(tf.keras.layers.Dense(n_classes, activation=activation, name='fc_out'))\n",
        "    return(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ced3afc-930a-4d3d-b1bc-2942eec4fb66",
      "metadata": {
        "id": "8ced3afc-930a-4d3d-b1bc-2942eec4fb66"
      },
      "outputs": [],
      "source": [
        "simple_cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6564b6-ffdc-4b52-ac17-309bacdfef15",
      "metadata": {
        "id": "ea6564b6-ffdc-4b52-ac17-309bacdfef15"
      },
      "outputs": [],
      "source": [
        "simple_cnn = simple_model(input_shape=(21,21,1))\n",
        "sgd = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.05, nesterov=True)\n",
        "simple_cnn.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "n_epochs = 20\n",
        "simple_model_history = simple_cnn.fit(train_data, train_class, epochs=n_epochs, batch_size=512, verbose=1, validation_data=(val_data, val_class), shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b778e737-c669-4660-89e6-cd761029560f",
      "metadata": {
        "id": "b778e737-c669-4660-89e6-cd761029560f"
      },
      "outputs": [],
      "source": [
        "model_history = simple_model_history\n",
        "import matplotlib.pyplot as plt\n",
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "acc = model_history.history['accuracy']\n",
        "val_acc = model_history.history['val_accuracy']\n",
        "plt.figure()\n",
        "plt.plot(np.arange(n_epochs), loss, label='Training Loss')\n",
        "plt.plot(np.arange(n_epochs), val_loss, label='Validation Loss')\n",
        "plt.title('Loss Curves')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "#plt.savefig('Keras_Loss.png')\n",
        "plt.figure()\n",
        "plt.plot(np.arange(n_epochs), acc, label='Training Accuracy')\n",
        "plt.plot(np.arange(n_epochs), val_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Curves')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "#plt.savefig('figures/ResNet50_LC.png')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}